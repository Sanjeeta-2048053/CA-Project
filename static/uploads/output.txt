frontiers REVIEW published: 28 February 2020 in Artificial Intelligence doi: 10.3389/frai.2020.00004 Check for updates An Introductory Review of Deep Learning for Prediction Models With Big Data Frank Emmert-Streib 1,2* , Zhen Yang1 Han Feng 1,3 , Shailesh Tripathi1,3 and Matthias Dehmer3,4,5 1 Predictive Society and Data Analytics Lab, Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland, 2 Institute of Biosciences and Medical Technology, Tampere, Finland, 3 School of Management, University of Applied Sciences Upper Austria, Steyr, Austria, 4 Department of Biomedical Computer Science and Mechatronics, University for Health Sciences, Medical Informatics and Technology (UMIT), Hall in Tyrol, Austria, 5 College of Artificial Intelligence, Nankai University, Tianjin, China Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, OPEN ACCESS especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Edited by: Fabrizio Riguzzi, Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks University of Ferrara, Italy (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These Reviewed by: models form the major core architectures of deep learning models currently used Karthik Soman, University of California, San Francisco, and should belong in any data scientist's toolbox. Importantly, those core architectural United States building blocks can be composed flexibly- in an almost Lego-like manner-to build Arnaud Fadja Nguembang, new application-specific network architectures. Hence, a basic understanding of these University of Ferrara, Italy network architectures is important to be prepared for future developments in Al. *Correspondence: Frank Emmert-Streib Keywords: deep learning, artificial intelligence, machine learning, neural networks, prediction models, data v@bio-complexity.com science Specialty section: This article was submitted to 1. INTRODUCTION Machine Learning and Artificial Intelligence, We are living in the big data era where all areas of science and industry generate massive a section of the journal amounts of data. This confronts us with unprecedented challenges regarding their analysis and Frontiers in Artificial Intelligence interpretation. For this reason, there is an urgent need for novel machine learning and artificial Received: 24 October 2019 intelligence methods that can help in utilizing these data. Deep learning (DL) is such a novel Accepted: 31 January 2020 methodology currently receiving much attention (Hinton et al., 2006). DL describes a family of Published: 28 February 2020 learning algorithms rather than a single method that can be used to learn complex prediction Citation: models, e.g., multi-layer neural networks with many hidden units (LeCun et al., 2015). Importantly, Emmert-Streib F, Yang Z, Feng H, deep learning has been successfully applied to several application problems. For instance, a deep Tripathi S and Dehmer M (2020) An Introductory Review of Deep Learning learning method set the record for the classification of handwritten digits of the MNIST data set for Prediction Models With Big Data. with an error rate of 0.21% (Wan et al., 2013). Further application areas include image recognition Front. Artif. Intell. 3:4. (Krizhevsky et al., 2012a; LeCun et al., 2015), speech recognition (Graves et al., 2013), natural doi: 10.3389/frai.2020.00004 language understanding (Sarikaya et al., 2014), acoustic modeling (Mohamed et al., 2011) and Frontiers in Artificial Intelligence | www.frontiersin.org 1 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models computational biology (Leung et al., 2014; Alipanahi et al., 2015; TABLE 1 An overview of frequently used activation functions for neuron models. Zhang S. et al., 2015; Smolander et al., 2019a,b). Activation function Models of artificial neural networks have been used since <(x) p'(x) Values about the 1950s (Rosenblatt, 1957); however, the current wave Hyperbolic tangent tanh(x)= 1 d(x)2 (-1,1) of deep learning neural networks started around 2006 (Hinton et al., 2006). A common characteristic of the many variations Sigmoid S(X)=170 $(x)(1 <(x)) (0,1) of supervised and unsupervised deep learning models is that these models have many layers of hidden neurons learned, e.g., 0 for X < 0 0 for X < 0 ReLu R(x) = [0,00) by a Restricted Boltzmann Machine (RBM) in combination X for X > 0 1 for X > 0 with Backpropagation and error gradients of the Stochastic 0 for X < 0 Gradient Descent (Riedmiller and Braun, 1993). Due to the Heaviside function H(x) = s(x) [0,1] 1 for X > 0 heterogeneity of deep learning approaches a comprehensive discussion is very challenging, and for this reason, previous 1 for X < 0 reviews aimed at dedicated sub-topics. For instance, a bird's eye Signum function sgn(x) = 0 for X = 0 28(x) [-1,1] view without detailed explanations can be found in LeCun et al. 1 for X > 0 (2015), a historic summary with many detailed references in Schmidhuber (2015) and reviews about application domains, e.g., Softmax yi=cion aj (0,1) image analysis (Rawat and Wang, 2017; Shen et al., 2017), speech recognition (Yu and Li, 2017), natural language processing (Young et al., 2018), and biomedicine (Cao et al., 2018). biological neuron. It is interesting to note that this model did not In contrast, our review aims at an intermediate level, consider learning. providing also technical details usually omitted. Given the In 1949, the first idea about biologically motivated learning interdisciplinary interest in deep learning, which is part of in neural networks was introduced by Hebb (1949). Hebbian data science (Emmert-Streib and Dehmer, 2019a), this makes it learning is a form of unsupervised learning of neural networks. easier for people new to the field to get started. The topics we In 1957, the Perceptron was introduced by Rosenblatt (1957). selected are focused on the core methodology of deep learning The Perceptron is a single-layer neural network serving as a approaches including Deep Feedforward Neural Networks (D- linear binary classifier. In the modern language of ANNs, a FFNN), Convolutional Neural Networks (CNNs), Deep Belief Perceptron uses the Heaviside function as an activation function Networks (DBNs), Autoencoders (AEs), and Long Short-Term (see Table 1). Memory (LSTM) networks. Further network architectures which In 1960, the Delta Learning rule for learning a Perceptron we discuss help in understanding these core approaches. was introduced by Widrow and Hoff (1960). The Delta Learning This paper is organized as follows. In the section 2, we provide rule, also known as Widrow & Hoff Learning rule or the a historical overview of general developments of neural networks. Least Mean Square rule, is a gradient descent learning rule for Then in section 3, we discuss major architectures distinguishing updating the weights of the neurons. It is a special case of the neural networks. Thereafter, we discuss Deep Feedforward backpropagation algorithm. Neural Networks (section 4), Convolutional Neural Networks In 1968, a method called Group Method of Data Handling (section 5), Deep Belief Networks (section 6), Autoencoders (GMDH) for training neural networks was introduced by (section 7) and Long Short-Term Memory networks (section 8) Ivakhnenko (1968). These networks are widely considered the in detail. In section 9, we provide a discussion of important issues first deep learning networks of the Feedforward Multilayer when learning neural network models. Finally, this paper finishes Perceptron type. For instance, the paper (Ivakhnenko, 1971) used in section 10 with conclusions. a deep GMDH network with 8 layers. Interestingly, the numbers of layers and units per layer could be learned and were not fixed from the beginning. 2. KEY DEVELOPMENTS OF NEURAL In 1969, an important paper by Minsky and Papert (1969) NETWORKS: A TIME LINE was published which showed that the XOR problem cannot be learned by a Perceptron because it is not linearly separable. The history of neural networks is long, and many people This triggered a pause phase for neural networks called have contributed toward their development over the decades. the "AI winter." Given the recent explosion of interest in deep learning, In 1974, error backpropagation (BP) has been suggested to use it is not surprising that the assignment of credit for key in neural networks (Werbos, 1974) for learning the weighted in a developments is not uncontroversial. In the following, we were supervised manner and applied in Werbos (1981). However, the aiming at an unbiased presentation highlighting only the most method itself is older (see e.g., Linnainmaa, 1976). distinguished contributions. In 1980, a hierarchical multilayered neural network for In 1943, the first mathematical model of a neuron was visual pattern recognition called Neocognitron was introduced created by McCulloch and Pitts (1943). This model aimed at by Fukushima (1980). After the deep GMDH networks (see providing an abstract formulation for the functioning of a above), the Neocognitron is considered the second artificial NN neuron without mimicking the biophysical mechanism of a real that deserved the attribute deep. It introduced convolutional NNs Frontiers in Artificial Intelligence www.frontiersin.org 2 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models (today called CNNs). The Neocognitron is very similar to the architecture of modern, supervised, deep Feedforward Neural 2000 Networks (D-FFNN) (Fukushima, 2013). In 1982, Hopfield introduced a content-addressable memory query 1500 neural network, nowadays called Hopfield Network (Hopfield, DL CNN 1982). Hopfield Networks are an example for recurrent neural networks. 1000 DBN LSTM In 1986, backpropagation reappeared in a paper by Rumelhart AEN et al. (1986). They showed experimentally that this learning 500 MLP algorithm can generate useful internal representations and, hence, be of use for general neural network learning tasks. 0 In 1987, Terry Sejnowski introduced the NETtalk algorithm 2007 2010 2013 2016 (Sejnowski and Rosenberg, 1987). The program learned how to year pronounce English words and was able to improve over time. FIGURE 1 Number of publications in dependence on the publication year for In 1989, a Convolutional Neural Network was trained with DL, deep learning; CNN, convolutional neural network; DBN, deep belief the backpropagation algorithm to learn handwritten digits network; LSTM, long short-term memory; AEN, autoencoder; and MLP, (LeCun et al., 1989). A similar system was later used to read multilayer perceptron. The legend shows the search terms used to query the handwritten checks and zip codes, processing cashed checks in Web of Science publication database. The two dashed lines are scaled by a factor of 5 (deep learning) and 3 (convolutional neural network). the United States in the late 90s and early 2000s. Note: In the 1980s, the second wave of neural network research emerged in great part via a movement called connectionism (Fodor and Pylyshyn, 1988). This wave lasted until In 2012, Alex Krizhevsky won the ImageNet Large Scale the mid 1990s. Visual Recognition Challenge by using AlexNet, a Convolutional In 1991, Hochreiter studied a fundamental problem of any Neural Network utilizing a GPU and improved upon LeNet5 (see deep learning network, which relates to the problem of not being above) (LeCun et al., 1989). This success started a convolutional trainable with the backpropagation algorithm (Hochreiter, 1991). neural network renaissance in the deep learning community His study revealed that the signal propagated by backpropagation (see Neocognitron). either decreases or increases without bounds. In case of a decay, In 2014, generative adversarial networks were introduced in this is proportional to the depth of the network. This is now Goodfellow et al. (2014). The idea is that two neural networks known as the vanishing or exploding gradient problem. compete with each other in a game-like manner. Overall, this In 1992, a first partial remedy to this problem has been establishes a generative model that can produce new data. This suggested by Schmidhuber (1992). The idea was to pre-train a has been called "the coolest idea in machine learning in the last RNN in an unsupervised way to accelerate subsequent supervised 20 years" by Yann LeCun. learning. The studied network had more than 1,000 layers in the In 2019, Yoshua Bengio, Geoffrey Hinton, and Yann LeCun recurrent neural network. were awarded the Turing Award for conceptual and engineering In 1995, oscillatory neural networks have been introduced breakthroughs that have made deep neural networks a critical in Wang and Terman (1995). They have been used in various component of computing. applications like image and speech segmentation and generating The reader interested in a more detailed early history of neural complex time series (Wang and Terman, 1997; Hoppensteadt and networks is referred to Schmidhuber (2015). Izhikevich, 1999; Wang and Brown, 1999; Soman et al., 2018). In Figure 1, we show the evolution of publications related In 1997, the first supervised model for learning RNN was to deep learning from the Web of Science publication database. introduced by Hochreiter and Schmidhuber (1997), which was Specifically, the figure shows the number of publications in called Long Short-Term Memory (LSTM). A LSTM prevents the dependence on the publication year for DL, deep learning; decaying error signal problem between layers by making the CNN, convolutional neural network; DBN, deep belief network; LSTM networks "remember" information for a longer period LSTM, long short-term memory; AEN, autoencoder; and MLP, of time. multilayer perceptron. The two dashed lines are scaled by a factor In 1998, the Stochastic Gradient Descent algorithm (gradient- of 5 (deep learning) and 3 (convolutional neural network), i.e., based learning) was combined with the backpropagation overall, for deep learning we found the majority of publications algorithm for improving learning in CNN (LeCun et al., 1989). (in total 30,230). Interestingly, most of these are in computer As a result, LeNet-5, a 7-level convolutional network, was science (52.1%) and engineering (41.5%). In application areas, introduced for classifying hand-written numbers on checks. medical imaging (6.2%), robotics (2.6%), and computational In 2006, is widely considered a breakthrough year because in biology (2.5%) received most attention. These observations are Hinton et al. (2006) it was shown that neural networks called a reflection of the brief history of deep learning indicating that Deep Belief Networks can be efficiently trained by using a strategy the methods are still under development. called greedy layer-wise pre-training. This initiated the third In the following sections, we will discuss all of these methods wave of neural networks that made also the use of the term deep in more detail because they represent the core methodology of learning popular. deep learning. In addition, we present background information Frontiers in Artificial Intelligence | www.frontiersin.org 3 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models about general artificial neural networks as far as this is needed for a feedforward structure. In Figures 3A,B, we show examples for a better understanding of the DL methods. a shallow and a deep architecture. In general, the depth of a network denotes the number of non- 3. ARCHITECTURES OF NEURAL linear transformations between the separating layers whereas the NETWORKS dimensionality of a hidden layer, i.e., the number of hidden neurons, is called its width. For instance, the shallow architecture Artificial Neural Networks (ANNs) are mathematical models that in Figure 3A has a depth of 2 whereas Figure 3B has a depth have been motivated by the functioning of the brain. However, of 4 [total number of layers minus one (input layer)]. The required number to call a Feedforward Neural Network (FFNN) the models we discuss in the following do not aim at providing biologically realistic models. Instead, the purpose of these models architecture deep is debatable, but architectures with more than two hidden layers are commonly considered as deep (Yoshua, is to analyze data. 2009). 3.1. Model of an Artificial Neuron A Feedforward Neural Network, also called a Multilayer The basic entity of any neural network is a model of a neuron. In Perceptron (MLP), can use linear or non-linear activation Figure 2A, we show such a model of an artificial neuron. functions (Goodfellow et al., 2016). Importantly, there are no The basic idea of a neuron model is that an input, x, together cycles in the NN that would allow a direct feedback. Equation with a bias, b is weighted by, W, and then summarized together. (3) defines how the output of a MLP is obtained from the input The bias, b, is a scalar value whereas the input x and the weights (Webb and Copsey, 2011). W are vector valued, i.e., x E Rn and W € Rn with n € N (3) corresponding to the dimension of the input. Note that the bias term is not always present but is sometimes omitted. The sum Equation (3) is the discriminant function of the neural network of these terms, i.e., Z = wTx + b forms then the argument (Webb and Copsey, 2011). For finding the optimal parameters of an activation function, 0, resulting in the output of the one needs a learning rule. A common approach is to define an neuron model, error function (or cost function) together with an optimization algorithm to find the optimal parameters by minimizing the error y =0(2)=0(wfx+b). = (1) for training data. Considering only the argument of 0 one obtains a linear 3.3. Recurrent Neural Networks discriminant function (Webb and Copsey, 2011). The family of Recurrent Neural Network (RNN) models has The activation function, 0, (also known as unit function two subclasses that can be distinguished based on their signal or transfer function) performs a non-linear transformation processing behavior. The first contains finite impulse recurrent of z. In Table 1, we give an overview of frequently used networks (FRNs) and the second infinite impulse recurrent activation functions. networks (IIRNs). That difference is that a FRN is given by a The ReLU activation function is called Rectified Linear Unit or directed acyclic graph (DAG) that can be unrolled in time and rectifier (Nair and Hinton, 2010). The ReLU activation function replaced with a Feedforward Neural Network, whereas an IIRN is the most popular activation function for deep neural networks. is a directed cyclic graph (DCG) for which such an unrolling is Another useful activation function is the softmax function not possible. (Lawrence et al., 1997): 3.3.1. Hopfield Networks exi A Hopfield Network (HN) (Hopfield, 1982) is an example for a Yi = exj (2) FRN. A HN is defined as a fully connected network consisting of McCulloch-Pitt neurons. A McCulloch-Pitts neuron is a binary model with an activation function given by The softmax maps a n-dimensional vector X into a n-dimensional vector y having the property Eii Yi = 1. Hence, the components of y represent probabilities for each of the n elements. The S = sgn(x) = -1 for x < 0 +1 for X > 0 (4) softmax is often used in the final layer of a network. If the Heaviside step function is used as activation function, the neuron The activity of the neurons Xi, i.e., model is known as perceptron (Rosenblatt, 1957). Usually, the model neuron shown in Figure 2A is represented N in a more ergonomic way by limiting the focus on its key (5) elements. In Figure 2B, we show such a representation that highlights merely the input part. is either updated synchronously or asynchronously. To be 3.2. Feedforward Neural Networks precise, Xj refers to x:- and Xi to xit (time progression). In order to build neural networks (NNs), the neurons need to be Hopfield Networks have been introduced to serve as a model connected with each other. The simplest architecture of a NN is of a content-addressable ("associative") memory, i.e., for storing Frontiers in Artificial Intelligence | www.frontiersin.org 4 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models A B Input Input 21 T b the w, Output Output 102 W2 202 E y 32 y 03 ws 23 FIGURE 2 I (A) Representation of a mathematical artificial neuron model. The input to the neuron is summed up and filtered by activation function 0 (for examples see Table 1). (B) Simplified Representation of an artificial neuron model. Only the key elements are depicted, i.e., the input, the output, and the weights. A B Hidden layers Hidden layer Input Input Output Output 21 12 x V/ 1/1 2° 32 (/2 1/2 Ts 35 ns FIGURE 3 I Two examples for Feedforward Neural Networks. (A) A shallow FFNN. (B) A Deep Feedforward Neural Network (D-FFNN) with 3 hidden layers. patterns. In this case, it has been shown that the weights are 3.3.2. Boltzmann Machine obtained by A Boltzmann Machine (Hinton and Sejnowski, 1983) can be described as a noisy Hopfield network because it uses a probabilistic activation function P Wij = ti(k)tj(k) (6) k=1 = = (7) whereas P is the number of patterns, t(k) is the k-th pattern and whereas Xi is obtained as in Equation (5). This model is important ti(k) its i-th component. From Equation (6), one can see that the because it is one of the first neural networks that uses hidden weights are symmetrical. An interesting question in this context units (latent variables). For learning the weights, the Contrastive is what is the maximal value of P or P/N, called the network Divergence algorithm (see Algorithm 9) can be used to train capacity (here N is the total number of patterns). In Hertz et al. Boltzmann Machines. Put simply, Boltzmann Machines are (1991) it was shown that the network capacity is = 0.138. It neural networks consisting of two layers-a visible layer and a is interesting to note that the neurons in a Hopfield Network hidden layer. Each edge between the two layers is undirected, cannot be distinguished as input neurons, hidden neurons and implying that information can flow in a bi-directional way. The output neurons because at the beginning every neuron is an input whole network is fully connected, which means that each neuron neuron, during the processing every neuron is a hidden neuron in the network is connected to all other neurons via undirected and at the end every neuron is an output neuron. edges (see Figures 8A,B). Frontiers in Artificial Intelligence | www.frontiersin.org 5 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models 3.4. An Overview of Network Architectures learning. A drawback is an increase in imprecision. However, for There is a large variety of different network architectures used data sets with a large number of samples (big data), the speed as deep learning models. The following Table 2 does not aim to advantage outweighs this drawback. provide a comprehensive list, but it includes the most popular models currently used (Yoshua, 2009; LeCun et al., 2015). 5. CONVOLUTIONAL NEURAL NETWORKS It is interesting to note that some of the models in Table 2 are composed by other networks. For instance, CDBNs are based A Convolutional Neural Network (CNN) is a special Feedforward on RBMs and CNNs (Lee et al., 2009); DBMs are based on Neural Network utilizing convolution, ReLU and pooling layers. RBMs (Salakhutdinov and Hinton, 2009); DBNs are based on Standard CNNs are normally composed of several Feedforward RBMs and MLPs; dAEs are stochastic Autoencoders that can Neural Network layers including convolution, pooling, and fully- be stacked on top of each other to build stacked denoising connected layers. Autoencoders (SdAEs). Typically, in traditional ANNs, each neuron in a layer In the following sections, we discuss the major core is connected to all neurons in the next layer, whereas each architectures Deep Feedforward Neural Networks (D-FFNN), connection is a parameter in the network. This can result in a Convolutional Neural Networks (CNNs), Deep Belief Networks very large number of parameters. Instead of using fully connected (DBNs), Autoencoders (AEs), and Long Short-Term Memory layers, a CNN uses a local connectivity between neurons, i.e., a networks (LSTMs) in more detail. neuron is only connected to nearby neurons in the next layer. This can significantly reduce the total number of parameters in 4. DEEP FEEDFORWARD NEURAL the network. NETWORKS Furthermore, all the connections between local receptive fields and neurons use a set of weights, and we denote this set of weights It can be proven that a Feedforward Neural Network with one as a kernel. A kernel will be shared with all the other neurons hidden layer and a finite number of neurons can approximate that connect to their local receptive fields, and the results of these any continuous function on a compact subset of Rn (Hornik, calculations between the local receptive fields and neurons using 1991). This is called the universal approximation theorem. The the same kernel will be stored in a matrix denoted as activation reason for using a FFNN with more than one hidden layer map. The sharing property is referred to as weight sharing of is that the universal approximation theorem does not provide CNNs (Le Cun, 1989). Consequently, different kernels will result information on how to learn such a network, which turned out to in different activation maps, and the number of kernels can be be very difficult. A related issue that contributes to the difficulty adjusted with hyper-parameters. Thus, regardless of the total of learning such networks is that their width can become number of connections between the neurons in a network, the exponentially large. Interestingly, the universal approximation total number of weights corresponds only to the size of the local theorem can also be proven for FFNN with many hidden layers receptive field, i.e., the size of the kernel. This is visualized in and a bounded number of hidden neurons (Lu et al., 2017) Figure 4B, where the total number of connections between the for which learning algorithms have been found. Hence, D- two layers is 9 but the size of the kernel is only 3. FFNNs are used instead of (shallow) FFNNs for practical reasons By combining weight sharing and the local connectivity of learnability. property, a CNN is able to handle data with high dimensions. Formally, the idea of approximating an unknown function f* See Figure 4A for a visualization of a CNN with three hidden can be written as layers. In Figure 4A, the red edges highlight the locality property of hidden neurons, i.e., only very few neurons (8) connect to the succeeding layers. This locality property of CNN makes the network sparse compared to a FFNN which is Here f is a function from a specific family that depends on the fully connected. parameters A, and is a non-linear activation function with one layer. For many hidden layers $ has the form 5.1. Basic Components of CNN 5.1.1. Convolutional Layer ). (9) A convolutional layer is an essential part in building a convolutional neural network. Similar to a hidden layer of Instead of guessing the correct family of functions from which f an ordinary neural network, a convolutional layer has the should be chosen, D-FFNNs learn this function by approximating same goal, which is to convert the input into a representation it via 6, which itself is approximated by the n hidden layers. of a more abstract level. However, instead of using a full The practical learning of the parameters of a D-FFNN connectivity, the convolutional layer uses a local connectivity (see Figure 3B) can be accomplished with the backpropagation to perform the calculations between input and the hidden algorithm, although for computational efficiency nowadays neurons. A convolutional layer uses at least one kernel to slide the Stochastic Gradient Descent is used (Bottou, 2010). The across the input, performing a convolution operation between Stochastic Gradient Descent calculates a gradient for a set of each input region and the kernel. The results are stored in randomly chosen training samples (batch) and updates the the activation maps, which can be seen as the output of parameters for this batch sequentially. This results in a faster the convolutional layer. Importantly, the activation maps can Frontiers in Artificial Intelligence www.frontiersin.org 6 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models TABLE 2 List of popular deep learning models, available learning algorithms (unsupervised, supervised) and software implementations in R or python. Model Unsupervised Supervised Software Autoencoder Keras (Chollet, 2015), R: dimRed (Kraemer et al., 2018), h2o (Candel et al., 2015), RcppDL (Kou and Sugomori, 2014) Convolutional Deep Belief Network (CDBN) R & python: TensorFlow (Abadi et al., 2016), Keras (Chollet, 2015), h2o (Candel et al., 2015) Convolutional Neural Network (CNN) R& python: Keras (Chollet, 2015) MXNet (Chen et al., 2015), Tensorflow (Abadi et al., 2016), h20 (Candel et al., 2015), fastai (python) (Howard and Gugger, 2018) Deep Belief Network (DBN) RcppDL (R) (Kou and Sugomori, 2014), python: Caffee (Jia et al., 2014), Theano (Theano Development Team, 2016), Pytorch (Paszke et al., 2017), R & python: TensorFlow (Abadi et al., 2016), h20 (Candel et al., 2015) Deep Boltzmann Machine (DBM) python: boltzmann-machines (Bondarenko, 2017), pydbm (Chimera, 2019) Denoising Autoencoder (dA) Tensorflow (R, python) (Abadi et al., 2016), Keras (R, python) (Chollet, 2015), RcppDL (R) (Kou and Sugomori, 2014) Long short-term memory (LSTM) rnn (R) (Quast, 2016), OSTSC (R) (Dixon et al., 2017), Keras (R and python) (Chollet, 2015), Lasagne (python) (Dieleman et al., 2015), BigDL (python) (Dai et al., 2018), Caffe (python) (Jia et al., 2014) Multilayer Perceptron (MLP) SparkR (R) (Venkataraman et al., 2016), RSNNS (R) (Bergmeir and Benítez, 2012), keras (R and python) (Chollet, 2015), sklearn (python) (Pedregosa et al., 2011), tensorflow (R and python) (Abadi et al., 2016) Recurrent Neural Network (RNN) RSNNS (R) (Bergmeir and Benítez, 2012), rnn (R) (Quast, 2016), keras (R and python) (Chollet, 2015) Restricted Boltzmann Machine (RBM) RcppDL (R) (Kou and Sugomori, 2014), deepnet (R) (Rong, 2014), pydbm (python) (Chimera, 2019), sklearn (python) (Chimera, 2019), Pylearn2 (Goodfellow et al., 2013), TheanoLM (Enarvi and Kurimo, 2016) Hidden layers A B Input Hidden T W1 Input Output w . 32 us (1) 151 T3 7/2 wag us W7 W2 Es W73 FIGURE 4 I (A) An example for a Convolutional Neural Network. The red edges highlight the fact that hidden layers are connected in a "local" way, i.e., only very few neurons connect the succeeding layers. (B) An example for shared weights and local connectivity in CNN. The red edges highlight the fact that hidden layers are connected in a "local" way, i.e., only very few neurons connect the succeeding layers. The labels W1, W2, W3 indicate the assigned weight for each connection, three hidden nodes share the same set of weights W1, W2 W3 when connecting to three local patches. contain features extracted by different kernels. Each kernel 1. Size of kernels (N). Each kernel has a window size, which is can act as a feature extractor and will share its weights with also referred to as receptive field. The kernel will perform a all neurons. convolution operation with a region matching its window size For the convolution process, some spatial arguments need to from the input, and produce results in its activation map. be defined in order to produce the activation maps of a certain 2. Stride (S). This parameter defines the number of pixels the size. Essential attributes include: kernel will move for the next position. If it is set to 1, each Frontiers in Artificial Intelligence | www.frontiersin.org 7 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models kernel will make convolution operations around the input resulting in a smaller input by conserving as much information volume and then shift 1 pixel at a time until it reaches the as possible. Also, a pooling layer is able to introduce spatial specified border of the input. Hence, the stride can be used invariance into the network (Scherer et al., 2010), which can help to downsize the dimension of the activation maps as the larger to improve the generalization of the model. In order to perform the stride the smaller the activation maps. pooling, a pooling layer uses stride, zero-padding, and a pooling 3. Zero-padding (P). This parameter is used to specify how many window size as hyper-parameters. The pooling layer will scan the zeros one wants to pad around the border of the input. This is entire input with the specified pooling window size in the same very useful for preserving the dimension of the input. manner as the kernel in a convolutional layer. For instance, using a stride of 2, window size of 2 and 0 zeros-padding for pooling These three parameters are the most common hyper-parameters will half the size of the input dimension. used for controlling the output volume of a convolutional layer. There are many types of pooling methods, e.g., averaging- Specifically, for an input of dimension Winput X Hinput X Z, for pooling, min-pooling and some advanced pooling methods, such the hyper-parameters size of the kernel (N), Stride (S), and Zero- as fractional max-pooling and stochastic pooling. The most padding (P) the dimension of the activation map, i.e., Wout X common used pooling method is max-pooling, as it has been Hout X D can be calculated by: shown to be superior in dealing with images by capturing invariances efficiently (Scherer et al., 2010). Max-pooling extracts Wout = (Winput N + 2P) the maximum value within each specified sub-window across the 1 activation map. The max-pooling can be formulated as Ai,j,k = (10) max(Ri j+n.k), where Ai,j,k is the maximum activation Hout = S+ value from the matrix R of size n X n centered at index i, j in the kth activation map with n is the window size. D = Z An example of how to calculate the result between an input 5.1.3. Fully-Connected Layer matrix and a kernel can be seen in Figure 5. A fully-connected layer is the basic hidden layer unit in The shared weights and the local connectivity help FFNN (see section 3.2). Interestingly, also for traditional CNN significantly in reducing the total number of parameters of architectures, a fully connected layer is often added between the network. For example, assuming that an input has dimension the penultimate layer and the output layer to further model 100 X 100 X 3, and that the convolutional layer and the number non-linear relationships of the input features (Krizhevsky et al., of kernels is 2 and each kernel has a local receptive field of size 2012b; Simonyan and Zisserman, 2014; Szegedy et al., 2015). 4, then the dimension of each kernel is 4 X 4 X 3 (3 is the depth However, recently the benefit of this has been questioned because of the kernel which will be the same as the depth of the input of the many parameters introduced by this, leading potentially to volume). For 100 neurons in the layer there will be in total only overfitting (Simonyan and Zisserman, 2014). As a result, more and more researchers started to construct CNN architecture 4 X 4 X 3 X 2 = 96 parameters in this layer because all the 100 neurons will share the same weights for each kernel. This without such a fully connected layer using other techniques like considers only the number of kernels and the size of the local max-over-time pooling (Lin et al., 2013; Kim, 2014) to replace the connectivity but does not depend on the number neurons in role of linear layers. the layer. In addition to reducing the number of parameters, shared 5.2. Important Variants of CNN weights and a local connectivity are important in processing 5.2.1. VGGNet images efficiently. The reason therefore is that local convolutional VGGNet (Simonyan and Zisserman, 2014) was a pioneer operations in an image result in values that contain certain in exploring how the depth of the network influences the characteristics of the image, because in images local values are performance of a CNN. VGGNet was proposed by the Visual generally highly correlated and the statistics formed by the local Geometry Group and Google DeepMind, and they studied values are often invariant in the location (LeCun et al., 2015). architectures with a depth of 19 (e.g., compared to 11 for AlexNet Hence, using a kernel that shares the same weights can detect Krizhevsky et al., 2012b). patterns from all the local regions in the image, and different VGG19 extended the network from eight weight layers (a kernels can extract different types of patterns from the image. structure proposed by AlexNet) to 19 weights layers by adding A non-linear activation function (for instance ReLu, tanh, 11 more convolutional layers. In total, the parameters increased sigmoid, etc.) is often applied to the values from the from 61 million to 144 million, however, the fully connected layer convolutional operations between the kernel and the input. These takes up most of the parameters. According to their reported values are stored in the activation maps, which will be later passed results, the error rate dropped from 29.6 to 25.5 regrading top- to the next layer of the network. 1 val.error (percentage of times the classifier did not give the correct class with the highest score) on the ILSVRC dataset, and 5.1.2. Pooling Layer from 10.4 to 8.0 regarding top-5 val.error (percentage of times A pooling layer is usually inserted between a convolutional layer the classifier did not include the correct class among its top and the following layer. Pooling layers aim at reducing the 5) on the ILSVRC dataset in ILSVRC2014. This indicates that dimension of the input with some pre-specified pooling method, a deeper CNN structure is able to achieve better results than Frontiers in Artificial Intelligence | www.frontiersin.org 8 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models 1 0 0 0 0 0 k1x0x1 0 1 0 0 1 1 3 0 2 2 x08180 1 0 1 0 0 1 0 1 1 0 3 2 3 x1x0x1 * 0 1 0 = 0 0 0 1 1 0 3 1 5 4 1 0 1 1 0 1 1 1 0 0 3 2 3 0 1 0 0 0 1 Input matrix of 6 X 6 Kernel of 3 X 3 Activation map FIGURE 5 I An example for calculating the values in the activation map. Here, the stride is 1 and the zero-padding is 0. The kernel slides by 1 pixel at a time from left to right starting from the left top position, after reaching the boarder the kernel will start from the second row and repeat the process until the whole input is covered. The red area indicates the local patch to be convoluted with the kernel, and the result is stored in the green field in the activation map. shallower networks. In addition, they stacked multiple 3 X 3 this sparse structure introduced by an inception block requires convolutional layers without a pooling layer placed in between much fewer parameters and, hence, is much more efficient. to replace the convolutional layer with a large filter sizes, e.g., 7 By stacking the inception structure throughout the network, X 7 or 11 X 11. They suggested such an architecture is capable of GoogLeNet won first place in the classification task of receiving the same receptive fields as those composed of larger ILSVRC2014, demonstrating the quality of the inception filter sizes. Consequently, two stacked 3 X 3 layers can learn structure. Followed by the inception v1, inception v2, v3, and features from a 5 X 5 receptive field, but with less parameters and the latest version v4 were introduced. Each generation introduced more non-linearity. some new features, making the network faster, more light-weight and more powerful. 5.2.2. GoogLeNet With Inception 5.2.3. ResNet The most intuitive way for improving the performance of a In principle, CNNs with a deeper structure perform better than Convolutional Neural Network is to stack more layers and add shallow ones (Simonyan and Zisserman, 2014). In theory, deeper more parameters to the layers (Simonyan and Zisserman, 2014). networks have a better ability to represent high level features However, this will impose two major problems. One is that too from the input, therefore improving the accuracy of predictions many parameters will lead to overfitting, and the other is that the (Donahue et al., 2014). However, one cannot simply stack more model becomes hard to train. and more layers. In the paper (He et al., 2016), the authors GoogLeNet (Szegedy et al., 2015) was introduced by observed the phenomena that more layers can actually hurt the Google. Until the introduction of inception, traditional state- performance. Specifically, in their experiment, network A had of-the-art CNN architectures mainly focused on increasing N layers, and network B had N + M layers, while the initial the size and depth of the neural network, which also N layers had the same structure. Interestingly, when training increased the computation cost of the network. In contrast, on the CIFAR-10 and ImageNet dataset, network B showed a GoogLeNet introduced an architecture to achieve state-of-the-art higher training error than network B. In theory, the extra M performance with a light-weight network structure. layers should result in a better performance, but instead they The idea underlying an inception network architecture is to obtained higher errors which cannot be explained by overfitting. keep the network as sparse as possible while utilizing the fast The reason for this is that the loss is getting optimized to local matrix computation feature provided by a computer. This idea minima, which is different to the vanishing gradient phenomena. facilitates the first inception structure (see Figure 6). This is referred to as the degradation problem (He et al., 2016). As one can see in the Figure 6, several parallel layers including ResNet (He et al., 2016) was introduced to overcome the 1 X 1 convolution and 3 X 3 max pooling operate at the degradation problem of CNNs to push the depth of a CNN same level on the input. Each tunnel (namely one separated to its limit. In (He et al., 2016), the authors proposed a novel sequential operation) has a different child layer, including 3 X 3 structure of a CNN, which is in theory capable of being extended convolutions, 5 X 5 convolutions and 1 X 1 convolution layer. to an infinite depth without losing accuracy. In their paper, they All the results from each tunnel are concatenated together at proposed a deep residual learning framework, which consists of the output layer. In this architecture, a 1x1 convolution is used multiple residual blocks to address the degradation problem. The to downscale the input image while reserving input information structure of a residual block is shown in the Figure 7. (Lin et al., 2013). They argued that concatenating all the features Instead of trying to learn the desired underlying mapping extracted by different filters corresponds to the idea that image H(x) from each few stacked layers, they used an identity information should be processed at different scales and only the mapping for input X from input to the output of the layer, aggregated features should be sent to the next level. Hence, the and then let the network learn the residual mapping F(x) = next level can extract features from different scales. Moreover, H(x) - x. After adding the identity mapping, the original Frontiers in Artificial Intelligence www.frontiersin.org 9 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models Input layer 1x1 convolutions 1x1 convolutions 3x3 max pooling 1x1 convolutions 3x3 convolutions 5x5 convolutions Ixl convolutions Filter concatenation FIGURE 6 Inception block structure. Here multiple blocks are stacked on top of each other, forming the input layer for the next block. one RBM is used sequentially. This adds to the depth of X the DBN. Due to the different nature of the networks RBM and D- weight layer FFNN, two different types of learning algorithms are used. F(X) Identity Practically, the Restricted Boltzmann Machines are used for ReLu mapping initializing a model in an unsupervised way. Thereafter, a weight of X supervised method is applied for the fine tuning of the parameters layer (Yoshua, 2009). In the following, we describe these two phases of the training of a DBN in more detail. F(X) + X ReLu 6.1. Pre-training Phase: Unsupervised Theoretically, neural networks can be learned by using FIGURE 7 The structure of a residual block. Inside a block there can be as supervised methods only. However, in practice it was found many weight layers as desired. that such a learning process can be very slow. For this reason, unsupervised learning is used to initialize the model parameters. The standard neural network learning algorithm mapping can be reformulated as H(x) = F(x) + X. The (backpropagation) was initially only able to learn shallow identity mapping is realized by making shortcut connections architectures. However, by using a Restricted Boltzmann from the input node directly to the output node. This can help Machine for the unsupervised initialization of the parameters one to address the degradation problem as well as the vanishing obtains a more efficient training of the neural network (Hinton (exploding) gradient issue of deep networks. In extreme cases, et al., 2006). deeper layers can just learn the identity map of the input A Restricted Boltzmann Machine is a special type of a to the output layer, by simply calculating the residuals as 0. Boltzmann Machine (BM), see section 3.3.2. The difference This enables the ability for a deep network to perform at least between a Restricted Boltzmann Machine and a Boltzmann not worse than shallow ones. Also, in practice, the residuals Machine is that Restricted Boltzmann Machines (RBMs) are never 0, which makes it possible for very deeper layers have constraints in the connectivity of their structure to always learn something new from the residuals therefore (Fischer and Igel, 2012). Specifically, there can be no producing better results. The implementation of ResNet helped connections between nodes in the same layer. For an example, to push the layers of CNNs to 152 by stacking so-called see Figure 8C. residual blocks through out the network. ResNet achieved the The values of neurons, V, in the visible layer are known, but best result in the ILSVRC2016 competition, with an error rate the neuron values, h, in the hidden layer are unknown. The of 3.57. parameters of the network are learned by defining an energy function, E, of the model which is then minimized. Frequently, a RBM is used with binary values, i.e., Vi € {0,1 6. DEEP BELIEF NETWORKS and hi € {0, 1}. The energy function for such a network is given by (Hinton, 2012): A Deep Belief Network (DBN) is a model that combines different types of neural networks with each other to form a new neural network model. Specifically, DBNs integrate m n m n Restricted Boltzmann Machines (RBMs) with Deep Feedforward h) = - aivi - - (11) Neural Networks (D-FFNN). The RBMs form the input unit j i whereas the D-FFNNs form the output unit. Frequently, RBMs are stacked on top of each other, which means more than whereas R = {a, b, W} is the set of model parameters. Frontiers in Artificial Intelligence www.frontiersin.org 10 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models A Visible B Visible Hidden Us U1 U2 18.1 214 U2 U1 Hidden Us hy ha 12 U4 c Visible Hidden Visible Hidden U1 U1 83 Ug U2 no Ug las Us FIGURE 8 | Examples for Boltzmann Machines. (A) The neurons are arranged on a circle. (B) The neurons are separated according to their type. Both Boltzmann Machines are identical and differ only in their visualization. (C) Transition from a Boltzmann Machine (left) to a Restricted Boltzmann Machine (right). Each configuration of the system corresponds to a parameters need to be found numerically. For this, the gradient probability defined via the Boltzmann distribution in of the log-likelihood is a typical approach for estimating the Equation (11): optimal parameters: p(v,h) = (12) aL(0t + In Equation (12), Z is the partition function given by: In Equation (16), the constant, n, in front of the gradient is (13) the learning rate and the first regularization term, -10(1, is v,h the weight-decay. The weight-decay is used to constrain the The probability for the network assigning to a visible vector V is optimization problem by penalizing large values of 0 (Hinton, given by summing over all possible hidden vectors: 2012). The parameter A is also called the weight-cost. The second regularization term in Equation (16) is called momentum. The purpose of the momentum is to make learning faster and to (14) reduce possible oscillations. Overall, this should stabilize the h learning process. For the optimization, the Stochastic Gradient Ascent Maximum-likelihood estimation (MLE) is used for estimating (SGA) is utilized using mini-batches. That means one selects the optimal parameters of the probabilistic model (Hayter, 2012). randomly a number of samples from the training set, k, For a training data set D = Dtrain = {V1: VI] consisting which are much smaller than the total sample size, and of l patterns, assuming that the patterns are iid (independent then estimates the gradient. The parameters, 0, are then and identical) distributed, the log-likelihood function is updated for the mini-batch. This process is repeated iteratively given by: until an epoch is completed. An epoch is characterized by using the whole training set once. A common problem l l is encountered when using mini-batches that are too L(0) = In = In p(vil() = (15) large, because this can slow down the learning process i=1 considerably. Frequently, k is chosen between 10 and 100 (Hinton, 2012). For simple cases, one may be able to find an analytical solution for Before the gradient can be used, one needs to approximate Equation (15) by solving of a In ((A)D) = 0. However, usually the the gradient of Equation (16). Specifically, the derivatives Frontiers in Artificial Intelligence | www.frontiersin.org 11 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models with respect to the parameters can be written in the In Equation (20), Oi = $(xi) is the ¡th output from the network following form: function $ : R Rn given the ith input Xi from the training set D = D train = {(x1,1 t1) (x1, ti)} and ti is the target output. Similarly, for maximizing the log-likelihood function of a ac(o)v) @wij = p(Hj = 1|v)vi - = RBM (see Equation 16), one uses gradient descent to find the ac(A)v) da = Vi - vp(v)vi (17) parameters that minimize the error function. ac(o)v) abj = p(Hj = 1|v) - = 3E In Equation (17), Hi denotes the value of hidden unit i and p(v) = V is the probability defined in Equation (14). For the conditional probability, one finds Here, the parameters (n, A and v) have the same meaning as explained above. Again, the gradient is typically not used for the n entire training data D, but instead smaller batches are used via = O WijVi + (18) the Stochastic Gradient Descent (SGD). The gradient of the RBM log-likelihood can be approximated and correspondingly using the CD algorithm (see Figure 9A). For this, the backpropagation algorithm is used (LeCun et al., 2015). Let us denote by ai the activation of the ith unit in the lth layer m = = (19) (l € {2, L}), b the corresponding bias and W1 ij the weight for the edge between the jth unit of the (l - 1) th layer and the ith i=1 unit of the lth layer. For activation function, 9, the activation Using the above equations in the presented form would be of the lth layer with the (1-1)tl layer as input is al = (()(")) = inefficient because these equations require a summation over + b(b). all visible vectors. For this reason, the Contrastive Divergence Application of the chain rule leads to (Nielsen, 2015): (CD) method is used for increasing the speed for the estimation of the gradient. In Figure 9A, we show pseudocode of the = CD algorithm. = The CD uses Gibbs sampling for drawing samples from aE (22) conditional distributions, so that the next value depends only on the previous one. This generates a Markov chain (Hastie et al., 2009). Asymptotically, for k 8 the distribution becomes the true stationary distribution. In this case, the CD ML. Interestingly, already k = 1 can lead to satisfactory In Equation (22), the vector gL contains the errors of the output approximations for the pre-training (Carreira-Perpinan and layer (L), whereas the vector gl contains the errors of the lth layer. Hinton, 2005). Here, indicates the element-wise product of vectors. From this In general, pre-training of DBNs consists of stacking RBMs. the gradient of the error of the output layer is given by That means the next RBM is trained using the hidden layer of the previous RBM as visible layer. This initializes the parameters for = (23) each layer (Hinton and Salakhutdinov, 2006). Interestingly, the order of this training is not fixed but can vary. For instance, first, the last layer can be trained and then the remaining layers can be In general, the result of this depends on E. For instance, for the trained (Hinton et al., 2006). In Figure 10, we show an example MSE we obtain = (aj-tj). As a result, the pseudocode 3E for the stacking of RBMs. aa,,1) for the backpropagation algorithm can be formulated as shown 6.2. Fine-Tuning Phase: Supervised in Figure 9B (Nielsen, 2015). The estimated gradients from After the initialization of the parameters of the neural network, as Figure 9B are then used to update the parameters (weights described in the previous step, these can now be fine-tuned. For and biases) via SGD (see Equation 21). More updates are this step, a supervised learning approach is used, i.e., the labels of performed using mini-batches until all training data have been the samples, omitted in the pre-training phase, are now utilized. used (Smolander, 2016). For learning the model, one minimizes an error function The resilient backpropagation algorithm (Rprop) is a (also called loss function or sometimes objective function). An modification of the backpropagation algorithm that was example for such an error function is the mean squared error originally introduced to speed up the basic backpropagation (MSE). (Bprop) algorithm (Riedmiller and Braun, 1993). There exist at least four different versions of Rprop (Igel and Hüsken, 2000) and in Algorithm 9 pseudocode for the iRprop algorithm n E = 2n 1 (20) (which improves Rprop with weight-backtracking) is shown (Smolander, 2016). i= Frontiers in Artificial Intelligence | www.frontiersin.org 12 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models A Input: RBM (with m visible and 73 hidden layers) and mini-batch D (sample size k) Output: Update Ab, for v € D do v for = 0, A - 1 do for 3 = do sample h, (t) Av p(hylvic) for i = do sample U, (++1) for i = 1 m, i = 1, n.do Awg + - - As + Ao, + -10 Abs - Ab, + p(H} - B Input: Mini-batch D (sample size k) Output: Update Ab, Aw for x E D do a(0,1) x for I € {2,3 L} do alxi) + g(x.l) for IE (L,L-1, - ,2} do * for I € Ab(2 + Aw(1 - Aw(1) +1 E g(x.1) c Input: Parametera e, Amas, (0) and epoch 1 Output: Update so for a do if DE (A-1) . on(t) > 0 then 700 700 - mini , + elseif ag(t-1) as(t) < : then 80 80 if EXt) > g(t-1) then - an(1) - 0 80 elseif ag(t-1) ag(t) 0 then B9 as sole) FIGURE 9 I (A) Contrastive Divergence k-step algorithm using Gibbs sampling. (B) Backpropagation algorithm. (C) iRprop+ algorithm. As one can see in Algorithm 9, iRprop+ uses information pre-training stage when the training data are sufficiently large about the sign of the partial derivative from time step (t - 1) to (LeCun et al., 2015). make a decision for the update of the parameter. Importantly, the In Figure 11, we show an example of the overall DBN learning results of comparisons have shown that the iRprop algorithm is procedure. The left-hand side shows the pre-training phase and faster than Bprop (Igel and Hüsken, 2000). the right-hand side the fine-tuning. It has been shown that the backpropagation algorithm with DBNs have been used successfully for many application tasks, SGD can learn good neural network models even without a e.g., natural language processing (Sarikaya et al., 2014), acoustic Frontiers in Artificial Intelligence www.frontiersin.org 13 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models 1st RBM 2nd RBM UI V1 kth RBM U2 172 U1 U3 # <> Up Us Um FIGURE 10 I Visualizing the stacking of RBMs in order to learn the parameters of a model in an unsupervised way. 5th RBM 50 50 4th RBM 125 125 1 3rd RBM 250 50 250 125 2nd RBM 500 250 500 500 1st RBM #Features #Features Pre- training Fine- tuning FIGURE 11 The two stages of DBN learning. (Left) The hidden layer (purple) of one RBM is the input of the next RBM. For this reason their dimensions are equal. (Right) The two edges in fine-tuning denote the two stages of the backpropagation algorithm: the input feedforwarding and the error backpropagation. The orange layer indicated the output. modeling (Mohamed et al., 2011), image recognition (Hinton The construction of Autoencoders is similar to DBNs. et al., 2006) and computational biology (Zhang S. et al., 2015). Interestingly, the original implementation of an autoencoder (Hinton and Salakhutdinov, 2006) pre-trained only the first half 7. AUTOENCODER of the network with RBMs and then unrolled the network, creating in this way the second part of the network. Similar to An Autoencoder is an unsupervised neural network model used DBNs, a pre-training phase is followed by a fine-tuning phase. In for representation learning, e.g., feature selection or dimension Figure 12, an illustration of the learning process is shown. Here, reduction. A common property of autoencoders is that the size the coding layer corresponds to the new encoding c providing, of the input and output layer is the same with a symmetric e.g., a reduced dimension of x. architecture (Hinton and Salakhutdinov, 2006). The underlying An Autoencoder does not utilize labels and, hence, it is an idea is to learn a mapping from an input pattern x to a new unsupervised learning model. In applications, the model has been encoding c = h(x), which ideally gives as output pattern the same successfully used for dimensionality reduction. Autoencoders as the input pattern, i.e., x = y = g(c). Hence, the encoding c, can achieve a much better two-dimensional representation of which has usually lower dimension than X, allows to reproduce array data, when an adequate amount of data is available (Hinton (or code for) x. and Salakhutdinov, 2006). Importantly, PCAs implement a linear Frontiers in Artificial Intelligence | www.frontiersin.org 14 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models #Features AFeatures W72 Deceder 500 500 I WG2 7 1 W2 7 50 250 250 Top RBM W 1 W3 1 W2 125 125 125 W42 w2 125 3th RBM Wa Code layer 250 W4 W4 250 125 125 2nd RBM W/2 W3 I W2 500 250 250 W2 I W2 500 500 500 Ist RBM W1 W1 Encoder W1 Weatures #Features AFeatures Fine- Pretraining Unrolling tuning FIGURE 12 Visualizing the idea of autoencoder learning. The learned new encoding of the input is represented in the code layer (shown in blue). transformation, whereas Autoencoders are non-linear. Usually, et al., 2002). A LSTM unit can remember values over arbitrary this results in a better performance. We would like to highlight time intervals and the three gates control the flow of information that there are many extensions of these models, e.g., sparse through the cell. The central feature of a LSTM cell is a part called autoencoder, denoising autoencoder or variational autoencoder "constant error carousel" (CEC) (Lipton et al., 2015). In general, (Vincent et al., 2010; Deng et al., 2013; Pu et al., 2016). a LSTM network is formed exactly like a RNN, except that the neurons in the hidden layers are replaced by memory blocks. 8. LONG SHORT-TERM MEMORY In the following, we discuss some core concepts and the corresponding technicalities (W and U stand for the weights and NETWORKS b for the bias). In Figure 14, we show a schematic description of a LSTM block with one cell. Long short-term memory (LSTM) networks were introduced by Hochreiter and Schmidhuber in 1997 (Hochreiter and Input gate: A unit with sigmoidal function that controls the Schmidhuber, 1997). LSTM is a variant of a RNN that has flow of information into the cell. It receives its activation from the ability to address the shortcomings of RNNs which do both output of the previous time h(t-1) and current input not perform well, e.g., when handling long-term dependencies x(t). Under the effect of the sigmoid function, an input gate it (Graves, 2013). Furthermore, LSTMs avoid the gradient generates values between zero and one. Zero indicates it blocks vanishing or exploding problem (Hochreiter, 1998; Gers et al., the information entirely, whereas values of one allow all the 1999). In 1999, a LSTM with a forget gate was introduced which information to pass. could reset the cell memory. This improved the initial LSTM and became the standard structure of LSTM networks (Gers = (24) et al., 1999). In contrast to Deep Feedforward Neural Networks, LSTMs contain feedback connections. Furthermore, they can Cell input layer: The cell input has a similar flow as the not only process single data points, such as vectors or arrays, but input gate, receiving h(t-1) and x(t) as input. However, a tanh sequences of data. For this reason, LSTMs are particularly useful activation is used to squish input values to a range between -1 for analyzing speech or video data. and 1 (denoted by It in Equation 25). It = (25) 8.1. LSTM Network Structure With Forget Gate Forget gate: A unit with a sigmoidal function determines Figure 13 shows an unrolled structure of a LSTM network model which information from previous steps of the cell should be (Wang et al., 2016). In this model, the input and output are memorized or forgotten. The forget gate ft assumes values organized vertically, while information is delivered horizontally between zero and one based on the input, h(t-1) and x(t) In over the time series. the next step, f is given by a Hadamard product with an old In a standard LSTM network, the basic entity is called LSTM cell state ct-1 to update to a new cell state c (Equation 26). In unit or a memory block (Gers et al., 1999). Each unit is composed this case, a value of zero means the gate is closed, so it will of a cell, the memory part of the unit, and three gates: an input completely forget the information of the old cell state ct-1, , gate, an output gate and a forget gate (also called keep gate) (Gers whereas values of one will make all information memorable. Frontiers in Artificial Intelligence | www.frontiersin.org 15 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models Output 30 Output 34-1 M 1++1 st+s Hidden h, Hidden he t+ Input I1 Input T Fr+ To+ Time t Time t 1 t t + 1 r + 2 FIGURE 13 | (Left) A folded structure of a LSTM network model. (Right) An unfolded structure of a LSTM network model. Xj is the input data at time i and yi is the corresponding output (i is the time step starting from (t - 1)). In this network, activated by softmax function is the final network output. Therefore, a forget gate has the right to reset the cell state if the old information is considered meaningless. n(4) = (26) CEC e tank Cell state: A cell state stores the memory of a cell over a longer 1 time period (Ming et al., 2017). Each cell has a recurrently self-connected linear unit which is called Constant Error X Carousel (CEC) (Hochreiter and Schmidhuber, 1997). The it 11 of CEC mechanism ensures that a LSTM network does not suffer (T tank a from the vanishing or exploding gradient problem (Elsayed et al., 2018). The CEC is regulated by a forget gate and it can also be reset by the forget gate. At time t, the current cell state ct is updated by the previous cell state ct-1 controlled by the forget gate and the product of the current input and the cell input, i.e., (it olt). Overall, Equation (27) describes the ACt-11 zit combined update of a cell state, FIGURE 14 Internal connectivity pattern of a standard LSTM unit (blue rectangle). The output from the previous time step, htt-1), and x1+) are the input (27) to the block at time t, then the output h(t) at time t will be an input to the same block in the next time step (t + 1). Output gate: A unit with a sigmoidal function can control the flow of information out of the cell. A LSTM uses the values of the output gate at time t (denoted by ot) to control the current cell state ct activated by a tanh function, to obtain the final In order to understand the base idea behind a Peephole LSTM, let output vector h(t), us assume the output gate ot-1 in a traditional LSTM network is closed. Then the output of the network h(t-1) at time (t - 1) will be 0, according to Equation (29), and in the next time step t, the = (28) regulating mechanism of all three gates will only depend on the ht = ot o tanh(c¹). (29) network input X(t-1) Therefore, the historical information will be lost completely. A Peephole LSTM avoids this problem by using a cell state instead of output h to control the gates. The following 8.2. Peephole LSTM equations describe a Peephole LSTM formally. A Peephole LSTM is a variant of a LSTM proposed by Gers and Schmidhuber (2000). In contrast to a standard LSTM discussed above, a Peephole LSTM uses the cell state c, instead = of h for regulating the forget gate, input gate and output (30) gate. In Figure 15, we show the internal connectivity of a I = + b') (31) Peephole LSTM unit whereas the red arrows represent the new ft = o + bf) (32) peephole connections. ot = o (w(ox), x(t) + U(oc) ct-1 + bo) (33) The key difference between a Peephole LSTM and a standard LSTM is that the forget gate f', input gate it and output gate ot do ct = f oct- 1 + it ol (34) not use h(t-1) as input. Instead, these gates use the cell state ct-1 ht = ot oct (35) Frontiers in Artificial Intelligence | www.frontiersin.org 16 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models called feature learning. This denotes a model that learns new and 6 better representations compared to the raw data. Importantly, deep learning models do not learn the final representation within one step but multiple ones corresponding to multi- CEC level representation transformations between the hidden layers (LeCun et al., 2015). - Another common property of deep learning models is that the subsequent transformations between layers are non-linear X 1 it 11 c-1 d (see Figure 3). This increases the expressive power of the model (Duda et al., 2000). Furthermore, individual representations are a (I tank a not designed manually, but learned via training data (LeCun et al., 2015). This makes deep learning models very flexible. 9.2. Differences Between Models Currently, CNNs are the dominating deep learning models for computer vision tasks (LeCun et al., 2015). They are effective when the data consist of arrays where nearby values in an FIGURE 15 | Internal connectivity of a Peephole LSTM unit (blue rectangle). array are correlated with each other, e.g., as is the case for Here x(t) is the input to the cell at time t, and h(t) is its output. The red arrows images, videos, and sound data. A convolutional layer can easily are the new peephole connections added, compared to the standard LSTM in Figure 14. process high-dimensional input by using the local connectivity and shared weights, while a pooling layer can down-sample the input without losing essential information. Each convolutional layer is capable of converting the input image into groups of more Aside from these main forms of LSTMs described above, there abstract features using different kernels; therefore, by stacking are further variants. For instance, a Bidirectional LSTM Network multiple convolution layers, the network is able to transform the (BLSTM) has been introduced by (Graves and Schmidhuber, input image to a representation that captures essential patterns 2005), which can access long-range context in both input from the input, thus making precise predictions. directions. Furthermore, in 2014, the concept of "Gated However, also in other areas, CNNs have shown very Recurrent Unit" was proposed, which is viewed as a simplified competitive results compared to other deep learning version of LSTM (Cho et al., 2014) and in 2015, Wai-kin architectures, e.g., in natural language processing (Kim, Wong and Wang-chun Woo introduced a Convolutional LSTM 2014; Yang et al., 2020). Specifically, CNNs can be good at Network (ConvLSTM) for precipitation nowcasting (Xingjian extracting local information from text and exploring meaningful et al., 2015). There are further variants of LSTM networks; semantic and syntactic meanings between phrases and words. however, most of them are designed for specific application Also, the natural composition of text data can be easily handled domains without clear performance advantage. by a CNN architecture. Hence, CNNs show very strong potential in performing classification tasks where successful predictions 8.3. Applications heavily rely on extracting key information from input text (Yin LSTMs have a wide range of applications in text generation, et al., 2017). text classification, language translation or image captioning The classical network architecture is fully connected and (Hwang and Sung, 2015; Vinyals et al., 2015). In Figure 16, an feedforward corresponding to a D-FFNN. Interestingly, in (Mayr LSTM classifier model for text classification is shown. In this et al., 2016), it has been shown that a D-FFNN outperformed figure, the input of the LSTM structure at each time step is a other methods for predicting the toxicity of drugs. Also for drug word embedding vector Vi, which is a common choice for text target predictions, a D-FFNN has been shown to be superior classification problems. A word embedding technique maps the compared to other methods (Mayr et al., 2018). This shows words or phrases in the vocabulary to vectors consisting of real that even such an architecture can be successfully used in numbers. Some common word embedding techniques include modern applications. word2vec, GloVe, FastText, etc. Zhou (2019). The output YN is Commonly, RNNs are used for problems with sequential the corresponding output at the Nth time step and Y'N is the final data, such as speech and language processing or modeling output after softmax activation of YN, which will determine the (Sundermeyer et al., 2012; Graves et al., 2013; Luong and classification of the input text. Manning, 2015). While DBNs and CNNs are feedforward networks, connections in RNNs can form cycles. This allows the 9. DISCUSSION modeling of dynamical changes over time (LeCun et al., 2015). A problem with finding the right application for a deep 9.1. General Characteristics of Deep learning model is that their application domains are not mutually Learning exclusive from each other. Instead, as the discussion above A property common to all deep learning models is that they shows, there is a considerable overlap and the best model can in perform so-called representation learning. Sometimes this is also many cases only be found by conducting a comparative study. Frontiers in Artificial Intelligence | www.frontiersin.org 17 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models IN IN LST. M1 LSTM2 LSTMN_1 LSTMN Vi V2 VN-1 VN FIGURE 16 I An LSTM classifier model for text classification. N is the sequence length of the input text (the number of words). Input from V1 to VN is a sequence of word embedding vectors used as input to the model at different time steps. Y'N is the final prediction result. TABLE 3 | Overview of applications of deep learning methods. Description DL type Application References Dermatologist-level classification of skin cancer with deep neural networks CNN Images Esteva et al., 2017 Deep learning for lung cancer prognostication: a retrospective multi-cohort radiomics study CNN Images Hosny et al., 2018 Character-level convolutional networks for text classification CNN Text Zhang X. et al. 2015 Recurrent convolutional neural networks for text classification CNN Text Lai et al., 2015 Comparing deep belief networks with support vector machines for classifying gene expression DBN Genomics Smolander et al., 2019a data from complex disorders Unsupervised feature learning for audio classification using convolutional deep belief networks C-DBN Audio Lee et al. 2009 Acoustic modeling using deep belief networks DBN Audio Mohamed et al., 2011 Jiang, M., et al. Text classification based on deep belief network and softmax regression DBN Text Jiang et al., 2018 Autoencoder for words AE Text Liou et al., 2014 Deep neural networks for learning graph representations AE Text Cao et al., 2016 Stacked denoising autoencoders: learning useful representations in a deep network with a SD-AE Images Vincent et al., 2010 local denoising criterion DeepCare: a deep dynamic memory model for predictive medicine LSTM Text Pham et al., 2016 Framewise phoneme classification with bidirectional LSTM and other neural network B-LSTM Audio Graves and Schmidhuber, 2005 architectures Deep sentence embedding using long short-term memory networks: analysis and application LSTM Text Palangi et al., 2016 to information retrieval Drug-drug interaction extraction from biomedical texts using long short-term memory network LSTM Text Sahu and Anand, 2018 In Table 3, we show several examples of different applications contrast, a prediction model is merely a black-box model for involving images, audio, text, and genomics data. making predictions. The models discussed in this review neither aim at providing physiological models of biological neurons nor offer an interpretable structure. Instead, they are prediction 9.3. Interpretable Models VS. Black-Box models. An example for a biologically motivated learning Models rule for neural networks is the Hebbian learning rule (Hebb, Any model in data science can be categorized either as an 1949). Hebbian learning is a form of unsupervised learning inferential model or a prediction model (Breiman, 2001; Shmueli, of neural networks that does not use global information 2010). An inferential model does not only make predictions about the error as backpropagation. Instead, only local but provides also an interpretable structure. Hence, it is a information is used from adjacent neurons. There are model of the prediction process itself, e.g., a causal model. In many extensions of Hebb's basic learning rule that have Frontiers in Artificial Intelligence www.frontiersin.org 18 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models 40- 30 20 10 0 0 50000 100000 150000 200000 250000 samples FIGURE 17 | Classification error of the EMNIST data in dependence on the number of training samples. The standard errors are shown in red and the horizontal dashed line corresponds to an error of 5% (reference). The results are averaged over 10 independent runs. been introduced based on new biological insights (see e.g., of thousands of samples but it is largely unclear how they Emmert-Streib, 2006). perform in a small data setting. This leaves it to the user to Recently, there is great interest in interpretable or explainable estimate learning curves of the generalization error for a given AI (XAI) (Biran and Cotton, 2017; Doshi-Velez and Kim, 2017). model to avoid spurious results (Emmert-Streib and Dehmer, Especially in the clinical and medical area, one would like to 2019b). have understandable decisions of statistical prediction models As an example to demonstrate this problem, we conducted because patients are affected (Holzinger et al., 2017). The field an analysis to explore the influence of the sample size on is still in its infancy, but if meaningful interpretations of general the accuracy of the classification of the EMNIST data. deep learning models could be found this would certainly EMNIST (Extended MNIST) (Cohen et al., 2017) consists revolutionize the field. of 280,000 handwritten characters (240, training samples As a note, we would like to add that the distinction between and 10,000 test samples) for 10 balanced classes (0-9). We an explainable AI model and a non-explainable model is not used a multilayered Long Short-Term Memory (LSTM) well-defined. For instance, the sparse coding model by Olshausen model for the 10-class handwritten digit classification and Field (1997) was shown to be similar to the coding of task. The model we used is a four-layer network (three images in the human visual cortex (Tosic and Frossard, 2011) hidden layers and one fully connected layer), and each and an application of this model can be found in Charles et al. hidden layer contains 200 neurons. For this analysis, we (2011), where an unsupervised learning approach was used to set the batch size to 100 and the training samples were learn an optimal sparse coding dictionary for the classification randomly drawn if the number of training samples was of high spectral imagery (HIS) data. Some may consider this < 240, 000 (subsampling). model as an XAI model because of the similarity to the working From the results in Figure 17, one can see that thousands mechanism of the human cortex, whereas others may question of training samples are needed to achieve a classification error this explanation. below 5% (blue dashed line). Specifically, more than 25, 000 training samples are needed. Given the relative simplicity 9.4. Big Data VS. Small Data of the problem-classification of ten digits, compared to classification or diagnosis of cancer patients-the severity of In statistics, the field of experimental design is concerned this issue should become clear. Also, these results show that with assessing if the available sample sizes are sufficient to a deep learning model cannot do miracles. If the number conduct a particular analysis (for a practical example see of samples is too small, the method breaks down. Hence, Stupnikov et al., 2016). In contrast, for all methods discussed the combination of a model and data is crucial for solving in this paper, we assumed that we are in the big data domain a task. implying sufficient samples. This corresponds to the ideal case. However, we would like to point out that for practical applications, one needs to assess this situation case-by-case to 9.5. Data Types ensure the available data (respectively the sample sizes) are A related problem to the sample size issue discussed above is sufficient to use deep learning models. Unfortunately, this issue the type of data. Examples for different data types are text data, is not well-represented in the current literature. As a rule- image data, audio data, network data or measurement/sensor of-thumb, deep learning models usually perform well for tens data (for instance from genomics) to name just a few. One Frontiers in Artificial Intelligence www.frontiersin.org 19 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models can further subdivide these data according to the application 10. CONCLUSION domain from which these originate, e.g., text data from medical publications, text data from social media or text data from In this paper, we provided an introductory review for deep novels. Considering such categorizations, it becomes clear that learning models including Deep Feedforward Neural Networks, the information content of 'one sample' does not have the (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief same meaning for each data type and for each application Networks (DBNs), Autoencoders (AE) and Long Short-Term domain. Hence, the assessment of deep learning models Memory networks (LSTMs). These models can be considered needs to be always conducted in a domain specific manner the core architectures that currently dominate deep learning. In because the transfer of knowledge between such models is not addition, we discussed related concepts needed for a technical straight forward. understanding of these models, e.g., Restricted Boltzmann Machines and resilient backpropagation. Given the flexibility 9.6. Further Advanced Models of network architectures allowing a "Lego-like" construction Finally, we would like to emphasize that there are additional of new models, an unlimited number of neural network but more advanced models of deep learning networks, which models can be constructed by utilizing elements of the core are outside the core architectures. For instance, deep learning architectural building blocks discussed in this review. Hence, a and reinforcement learning have been combined with each basic understanding of these elements is key to be equipped for other to form deep reinforcement learning (Mnih et al., 2015; future developments in AI. Arulkumaran et al., 2017; Henderson et al., 2018). Such models have found application in problems from robotics, games AUTHOR CONTRIBUTIONS and healthcare. Another example for an advanced model is a graph CNN, FE-S conceived the study. All authors contributed to all aspects which is particularly suitable when data have the form of graphs of the preparation and the writing of the manuscript. (Henaff et al., 2015; Wu al., 2019). Such models have been used in natural language processing, recommender systems, genomics FUNDING and chemistry (Li et al., 2018; Yao et al., 2019). Lastly, a further advanced model is a Variational Autoencoder MD thanks the Austrian Science Funds for supporting this work (VAE) (An and Cho, 2015; Doersch, 2016). Put simply, a VAR (project P 30031). is a regularized Autoencoder that uses a distribution over the latent spaces as encoding for the input, instead of a single ACKNOWLEDGMENTS point. The major application of VAE is as a generative model for generating similar data in an unsupervised manner, e.g., for We would like to thank Johannes Smolander for discussions image or text generation. about Deep Belief Networks. REFERENCES Carreira-Perpinan, M. A., and Hinton, G. E. (2005). "On contrastive divergence learning," in Proceedings of the Tenth International Workshop on Artificial Alipanahi, B., Delong, A., Weirauch, M. T., and Frey, B. J. (2015). Intelligence and Statistics (Citeseer), 33-40. Predicting the sequence specificities of DNA-and RNA-binding Charles, A. S., Olshausen, B. A., and Rozell, C. J. (2011). proteins by deep learning. Nat. Biotechnol. 33, 831-838. doi: 10.1038/n Learning sparse codes for hyperspectral imagery. IEEE J. bt.3300 Select. Top. Signal Process. 5, 963-978. doi: 10.1109/JSTSP.2011 An, J., and Cho, S. (2015). Variational Autoencoder Based Anomaly Detection Using 2149497 Reconstruction Probability. Special Lecture on IE 2. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., et al. (2015). Mxnet: a flexible Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). and efficient machine learning library for heterogeneous distributed systems. Deep reinforcement learning: a brief survey. IEEE Signal Process. Mag. 34, Chimera (2019). Pydbm. arXiv:1512.01274. 26-38. doi: 10.1109/MSP.2017.2743240 Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Bergmeir, C., and Benítez, J. M. (2012). Neural networks in R using the Schwenk, H., et al. (2014). Learning phrase representations using rnn encoder- stuttgart neural network simulator: RSNNS. J. Stat. Softw. 46, 1-26. decoder for statistical machine translation. arXiv [Preprint]. arXiv:1406.1078. doi: 10.18637/jss.v046.i07 doi: 10.3115/v1/D14-1179 Biran, O., and Cotton, C. (2017). "Explanation and justification in machine Chollet, F. (2015). Keras. Available online at: https://github.com/fchollet/keras learning: a survey," in IJCAI-17 Workshop on Explainable AI (XAI). Vol. 8, 1. Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. (2017). Emnist: an Bottou, L. (2010). "Large-scale machine learning with stochastic gradient descent," extension of mnist to handwritten letters. arXiv[Preprint]. arXiv:1702.05373. in Proceedings of COMPSTAT 2010 (Springer), 177-186. doi: 10.1109/IJCNN.2017.7966217 Breiman, L. (2001). Statistical modeling: the two cultures (with comments Dai, J., Wang, Y., Qiu, X., Ding, D., Zhang, Y., Wang, Y., et al. (2018). BigDL: a and a rejoinder by the author). Stat. Sci. 16, 199-231. doi: 10.1214/ss/ distributed deep learning framework for big data. arXiv:1804.05839. 1009213726 [Dataset] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al. Cao, C., Liu, F., Tan, H., Song, D., Shu, W., Li, W., et al. (2018). Deep learning (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed and its applications in biomedicine. Genomics Proteomics Bioinform. 16, 17-32. systems. arXiv:1603.04467. doi: 10.1016/j.gpb.2017.07.003 [Dataset] Bondarenko, Y. (2017). Boltzman-Machines. Cao, S., Lu, W., and Xu, Q. (2016). "Deep neural networks for learning graph [Dataset] Candel, A., Pramar, V., LeDell, E., and Arora, A. (2015). Deep Learning representations," in Thirtieth AAAI Conference on Artificial Intelligence. With H2O. Frontiers in Artificial Intelligence I www.frontiersin.org 20 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models [Dataset] Dieleman, S., Schlüter, J., Raffel, C., Olson, E., Sonderby, S. K., Nouri, D., Graves, A., Mohamed, A., and Hinton, G. E. (2013). "Speech recognition with deep et al. (2015). Lasagne: First Release. recurrent neural networks," in 2013 IEEE International Conference on Acoustics, [Dataset] Howard J., and Gugger S. (2018). fastai: A Layered API for Deep Learning. Speech and Signal Processing (ICASSP). arXiv:2002.04688. Graves, A., and Schmidhuber, J. (2005). Framewise phoneme classification with Deng, J., Zhang, Z., Marchi, E., and Schuller, B. (2013). "Sparse autoencoder-based bidirectional LSTM and other neural network architectures. Neural Netw. 18, feature transfer learning for speech emotion recognition," in 2013 Humaine 602-610. doi: 10.1016/j.neunet.2005.06.042 Association Conference on Affective Computing and Intelligent Interaction Hastie, T.J. Tibshirani, R. J., and Friedman, J. H. (2009). The Elements of Statistical (IEEE), 511-516. Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Dixon, M., Klabjan, D., and Wei, L. (2017). Ostsc: over sampling for time series Springer. classification in R. Hayter, H. O. (2012). Probability and Statistics for Engineers and Scientists. 4th Edn. Doersch, C. (2016). Tutorial on variational autoencoders. arXiv [Preprint]. Duxbury Press). arXiv:1606.05908. He, K., Zhang, X., Ren, S., and Sun, J. (2016). "Deep residual learning for image Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., recognition," in Proceedings of the IEEE Conference on Computer Vision and et al. (2014). "Decaf: a deep convolutional activation feature for generic Pattern Recognition, 770-778. visual recognition," in International Conference on Machine Learning, Hebb, D. (1949). The Organization of Behavior. New York, NY: Wiley. 647-655. Henaff, M., Bruna, J., and LeCun, Y. (2015). Deep convolutional networks on Doshi-Velez, F., and Kim, B. (2017). Towards a rigorous science of interpretable graph-structured data. arXiv [Preprint]. arXiv:1506.05163. machine learning. arXiv [Preprint]. arXiv:1702.08608. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Duda, R. O., Hart, P. E., and Stork, D. G. (2000). Pattern Classification. 2nd Edn. "Deep reinforcement learning that matters," in Thirty-Second AAAI Conference Wiley. on Artificial Intelligence. Elsayed, N., Maida, A. S., and Bayoumi, M. (2018). Reduced-gate convolutional Hertz, J., Krogh, A., and Palmer, R. (1991). Introduction to the Theory of Neural LSTM using predictive coding for spatiotemporal prediction. arXiv [Preprint]. Compuation. Addison-Wesley. arXiv:1810.07251. Hinton, G. E. (2012). Neural Networks: Tricks of the Trade. 2nd Edn. Chapter. Emmert-Streib, F. (2006). A heterosynaptic learning rule for neural A Practical Guide to Training Restricted Boltzmann Machines. Berlin; networks. Int. J. Mod. Phys. C 17, 1501-1520. doi: 10.1142/S0129183106 Heidelberg: Springer Berlin Heidelberg, 599-619. 009916 Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning Emmert-Streib, F., and Dehmer, M. (2019a). Defining data science by a data-driven algorithm for deep belief nets. Neural Comput. 18, 1527-1554. quantification of the community. Mach. Learn. Knowl. Extract. 1, 235-251. doi: 10.1162/neco.2006.18.7.1527 doi: 10.3390/make1010015 Hinton, G. E., and Salakhutdinov, R. R. (2006). Reducing the dimensionality Emmert-Streib, F., and Dehmer, M. (2019b). Evaluation of regression of data with neural networks. Science 313, 504-507. doi: 10.1126/science. models: model assessment, model selection and generalization error. 1127647 Mach. Learn. Knowl. Extract. 1, 521-551. doi: 10.3390/make10 Hinton, G. E., and Sejnowski, T. J. (1983). "Optimal perceptual inference," in 10032 Proceedings of the IEEE conference on Computer Vision and Pattern Recognition Enarvi, S., and Kurimo, M. (2016). TheanoLM-an extensible toolkit (Citeseer), 448-453. for neural network language modeling. Proc. Interspeech 3052-3056 Hochreiter, S. (1991). Untersuchungen zu Dynamischen Neuronalen Netzen. doi: 10.21437/Interspeech.2016-618 Diploma, Technische Universität München 91. Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., et al. (2017). Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent Dermatologist-level classification of skin cancer with deep neural networks. neural nets and problem solutions. Int. J. Uncertainty Fuzziness Knowl. Based Nature 542:115. doi: 10.1038/nature21056 Syst. 6, 107-116. Fischer, A., and Igel, C. (2012). "An introduction to restricted boltzmann Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural machines," in Progress in Pattern Recognition, Image Analysis, Computer Vision, Comput. 9, 1735-1780. and Applications (Springer), 14-36. Holzinger, A., Biemann, C., Pattichis, C. S., and Kell, D. B. (2017). What do we Fodor, J. A., and Pylyshyn, Z. W. (1988). Connectionism and cognitive need to build explainable AI systems for the medical domain? arXiv [Preprint]. architecture: a critical analysis. Cognition 28, 3-71. arXiv:1712.09923. Fukushima, K. (1980). Neocognitron: A self-organizing neural network model Hopfield, (1982). Neural networks and physical systems with emergent collective for a mechanism of pattern recognition unaffected by shift in position. Biol. computational abilities. Proc. Natl. Acad. Sci. U.S.A. 79, 2554-2558. Cybernet. 36, 193-202. Hoppensteadt, F. C., and Izhikevich, E. M. (1999). Oscillatory neurocomputers Fukushima, K. (2013). Training multi-layered neural network neocognitron. with dynamic connectivity. Phys. Rev. Lett. 82:2983. Neural Netw. 40, 18-31. doi: 10.1016/j.neunet.2013.01.001 Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Gers, F. A., and Schmidhuber, J. (2000). "Recurrent nets that time and count," in Neural Netw. 4, 251-257. Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Hosny, A., Parmar, C., Coroller, T. P., Grossmann, P., Zeleznik, R., Kumar, Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for A., et al. (2018). Deep learning for lung cancer prognostication: a the New Millennium (IEEE), Vol. 3, 189-194. retrospective multi-cohort radiomics study. PLoS Med. 15:e1002711. Gers, F. A., Schmidhuber, J., and Cummins, F. (1999). Learning to forget: doi: 10.1371/journal.pmed.1002711 continual prediction with LSTM. Neural Comput. 12, 2451-2471. Hwang, K., and Sung, W. (2015). "Single stream parallelization of generalized doi: 10.1162/089976600300015015 LSTM-like rnns on a GPU," in 2015 IEEE International Conference on Acoustics, Gers, F. A., Schraudolph, N. N., and Schmidhuber, J. (2002). Learning precise Speech and Signal Processing (ICASSP) (IEEE), 1047-1051. timing with lstm recurrent networks. J. Mach. Learn. Res. 3, 115-143. Available Igel, C., and Hüsken, M. (2000). "Improving the RPROP learning algorithm," online at: http://www.jmlr.org/papers/v3/gers02a.html in Proceedings of the Second International ICSC Symposium on Neural Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. Computation (NC 2000), Vol. 2000 (Citeseer), 115-121. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Ivakhnenko, A. G. (1968). The group method of data of handling; a rival of the et al. (2014). "Generative adversarial nets," in Advances in Neural Information method of stochastic approximation. Soviet Autom. Control 13, 43-55. Processing Systems, 2672-2680. Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Trans. Syst. Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Man Cybernet. SMC-1, 364-378. Pascanu, R., et al. (2013). Pylearn2: a machine learning research library. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., et al. (2014). arXiv:1308.4214. "Caffe: convolutional architecture for fast feature embedding," in Proceedings of Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv the 22Nd ACM International Conference on Multimedia, MM '14 (New York, [Preprint]. arXiv:1308.0850. NY: ACM), 675-678. Frontiers in Artificial Intelligence | www.frontiersin.org 21 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models Jiang, M., Liang, Y., Feng, X., Fan, X., Pei, Z., Xue, Y., et al. (2018). Text Nair, V., and Hinton, G. E. (2010). "Rectified linear units improve restricted classification based on deep belief network and softmax regression. Neural boltzmann machines," in Proceedings of the 27th International Conference on Comput. Appl. 29, 61-70. doi: 10.1007/s00521-016-2401-x Machine Learning (ICML-10), 807-814. Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv Nielsen, M. A. (2015). Neural Networks and Deep Learning. Determination Press. [Preprint]. arXiv:1408.5882. doi: 10.3115/v1/D14-1181 Olshausen, B. A., and Field, D. J. (1997). Sparse coding with an overcomplete basis Kou, Q., and Sugomori, Y. (2014). Rcppdl. set: a strategy employed by v1? Vision Res. 37, 3311-3325. Kraemer, G., Reichstein, M., and D., M. M. (2018). dimRed and Palangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J., et al. (2016). Deep sentence oRanking-unifying dimensionality reduction in R. R J. 10, 342-358. embedding using long short-term memory networks: Analysis and application doi: 10.32614/RJ-2018-039 to information retrieval. IEEE/ACM Trans. Audio Speech Lang. Process. 24, Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012a). ImageNet Classification 694-707. doi: 10.1109/TASLP.2016.2520371 with Deep Convolutional Neural Networks. Curran Associates, Inc. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., et al. (2017). Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012b). "Imagenet classification Automatic differentiation in pytorch. Available online at: https://www. with deep convolutional neural networks," in Advances in Neural Information semanticscholar.org/paper/Automatic-differentiation-in-PyTorch-Paszke Processing Systems, 1097-1105. Gross/b36a5bb1707bb9c70025294b3a310138aae8327a Lai, S., Xu, L., Liu, K., and Zhao, J. (2015). "Recurrent convolutional neural Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., networks for text classification," in Twenty-Ninth AAAI Conference on Artificial et al. (2011). Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, Intelligence. 2825-2830. Available online at: http://www.jmlr.org/papers/v12/pedregosalla Lawrence, S., Giles, C. L., Tsoi, A. C., and Back, A. D. (1997). Face recognition: a Pham, T., Tran, T., Phung, D., and Venkatesh, S. (2016). "Deepcare: a deep convolutional neural-network approach. IEEE Trans. Neural Netw. 8, 98-113. dynamic memory model for predictive medicine," in Pacific-Asia Conference Le Cun, Y. (1989). Generalization and Network Design Strategies. Technical on Knowledge Discovery and Data Mining (Springer), 30-41. Report CRG-TR-89-4, Connectionism in Perspective. University of Toronto Pu, Y., Gan, Z., Henao, R., Yuan, X., Li, C., Stevens, A., et al. (2016). "Variational Connectionist Research Group, Toronto, ON. autoencoder for deep learning of images, labels and captions," in Advances in LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521:436. Neural Information Processing Systems, 2352-2360. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Quast, B. (2016). RNN: A Recurrent Neural Network in R. Working Papers. et al. (1989). Backpropagation applied to handwritten zip code recognition. Rawat, W., and Wang, Z. (2017). Deep convolutional neural networks for Neural Comput. 1, 541-551. image classification: a comprehensive review. Neural Comput. 29, 2352-2449. Lee, H., Pham, P., Largman, Y., and Ng, A. Y. (2009). "Unsupervised feature doi: 10.1162/neco_a_00990 learning for audio classification using convolutional deep belief networks," in Riedmiller, M., and Braun, H. (1993). "A direct adaptive method for faster Advances in Neural Information Processing Systems, 1096-1104. backpropagation learning: the rprop algorithm," in IEEE International Leung, M. K. K., Xiong, H. Y., Lee, L. J., and Frey, B. J. (2014). Deep Conference on Neural Networks (IEEE), 586-591. learning of the tissue-regulated splicing code. Bioinformatics 30, 121-129. Rong, X. (2014). Deep Learning Toolkit in R. doi: 10.1093/bioinformatics/btu277 Rosenblatt, F. (1957). The Perceptron, A Perceiving and Recognizing Automaton Li, R., Wang, S., Zhu, F., and Huang, J. (2018). "Adaptive graph convolutional Project Para. Cornell Aeronautical Laboratory. neural networks," in Thirty-Second AAAI Conference on Artificial Intelligence. Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by Lin, M., Chen, Q., and Yan, S. (2013). Network in network. arXiv [Preprint]. back-propagating errors. Nature 323, 533-536. arXiv:1312.4400. Sahu, S. K., and Anand, A. (2018). Drug-drug interaction extraction from Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT biomedical texts using long short-term memory network. J. Biomed. Inform. Numer. Math. 16, 146-160. 86, 15-24. doi: 10.1016/j.jbi.2018.08.005 Liou, C.-Y., Cheng, W.-C., Liou, J.-W., and Liou, D.-R. (2014). Autoencoder for Salakhutdinov, R., and Hinton, G. E. (2009). "Deep boltzmann machines," in words. Neurocomputing 139, 84-96. doi: 10.1016/j.neucom.2013.09.05 International conference on artificial intelligence and statistics, 448-455. Lipton, Z. C., Berkowitz, J., and Elkan, C. (2015). A critical review of recurrent Sarikaya, R., Hinton, G. E., and Deoras, A. (2014). Application of deep belief neural networks for sequence learning. arXiv [Preprint]. arXiv:1506.00019. networks for natural language understanding. IEEE/ACM Trans. Audio Speech Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. (2017). "The expressive power of Lang. Process. 22, 778-784. doi: 10.1109/TASLP.2014.2303296 neural networks: a view from the width," in Advances in Neural Information Scherer, D., Müller, A., and Behnke, S. (2010). "Evaluation of pooling operations in Processing Systems, 6231-6239. convolutional architectures for object recognition," in International Conference Luong, M.-T., and Manning, C. D. (2015). "Stanford neural machine translation on Artificial Neural Networks (Springer), 92-101. systems for spoken language domains," in Proceedings of the International Schmidhuber, J. (1992). Learning complex, extended sequences using the principle Workshop on Spoken Language Translation, 76-79. of history compression. Neural Comput. 4, 234-242. Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). Schmidhuber, J. (2015). Deep learning in neural networks: an overview. Neural Deeptox: toxicity prediction using deep learning. Front. Environ. Sci. 3:80. Netw. 61, 85-117. doi: 10.1016/j.neunet.2014.09.003 doi: 10.3389/fenvs.2015.00080 Sejnowski, T. J., and Rosenberg, C. R. (1987). Parallel networks that learn to Mayr, A., Klambauer, G., Unterthiner, T., Steijaert, M., Wegner, J. K., pronounce english text. Complex Syst. 1, 145-168. Ceulemans, H., et al. (2018). Large-scale comparison of machine learning Shen, D., Wu, G., and Suk, H.-I. (2017). Deep learning in methods for drug target prediction on chembl. Chem. Sci. 9, 5441-5451. medical image analysis. Annu. Rev. Biomed. Eng. 19, 221-248. doi: 10.1039/C8SC00148K doi: 10.1146/annurev-bioeng-071516-044442 McCulloch, W., and Pitts, W. (1943). A logical calculus of the ideas immanent in Shmueli, G. (2010). To explain or to predict? Stat. Sci. 25, 289-310. nervous activity. Bull. Math. Biophys. 5, 115-133. doi: 10.1214/10-STS330 Ming, Y., Cao, S., Zhang, R., Li, Z., Chen, Y., Song, Y., et al. (2017). "Understanding Simonyan, K., and Zisserman, A. (2014). Very deep convolutional networks for hidden memories of recurrent neural networks," in 2017 IEEE Conference on large-scale image recognition. arXiv [Preprint]. arXiv:1409.1556. Visual Analytics Science and Technology (VAST) (IEEE), 13-24. Smolander, J. (2016). Deep learning classification methods for complex disorders Minsky, M., and Papert, S. (1969). Perceptrons. MIT Press. (Master's thesis), The School of the thesis, Tampere University of Technology, Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Tampere, Finland. Available online at: https://dspace.cc.tut.fi/dpub/handle/ et al. (2015). Human-level control through deep reinforcement learning. Nature 123456789/23845 518:529. doi: 10.1038/naturel4236 Smolander, J., Dehmer, M., and Emmert-Streib, F. (2019a). Comparing Mohamed, A.-R., Dahl, G. E., and Hinton, G. (2011). Acoustic modeling using deep belief networks with support vector machines for classifying gene deep belief networks. IEEE Trans. Audio Speech Lang. Process. 20, 14-22. expression data from complex disorders. FEBS Open Bio 9, 1232-1248 doi: 10.1109/TASL.2011.2109382 doi: 10.1002/2211-5463.12652 Frontiers in Artificial Intelligence | www.frontiersin.org 22 February 2020 | Volume 3 Article 4 Emmert-Streib et al. Deep Learning Models Smolander, J., Stupnikov, A., Glazko, G., Dehmer, M., and Emmert-Streib, F. Werbos, P. (1974). Beyond regression: new tools for prediction and analysis in (2019b). Comparing biological information contained in mRNA and non- the behavioral sciences (Ph.D. thesis), Harvard University, Harvard, MA, coding RNAs for classification of lung cancer patients. BMC Cancer 19:1176. United States. doi: 10.1186/s12885-019-6338-1 Werbos, P. J. (1981). "Applications of advances in nonlinear sensitivity analysis," Soman, K., Muralidharan, V., and Chakravarthy, V.S. (2018). An oscillatory neural in Proceedings of the 10th IFIP Conference, 31.8-4.9, New York, 762-770. autoencoder based on frequency modulation and multiplexing. Front. Comput. Widrow, B., and Hoff, M. E. (1960). Adaptive Switching Circuits. Technical Report, Neurosci. 12:52. doi: 10.3389/fncom.2018.00052 Stanford University, California; Stanford Electronics Labs. Stupnikov, A., Tripathi, S., de Matos Simoes, R., McArt, D., Salto-Tellez, Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S. (2019). M., Glazko, G., et al. (2016). samExploreR: exploring reproducibility and A comprehensive survey on graph neural networks. arXiv [Preprint]. robustness of RNA-seq results based on SAM files. Bioinformatics 32, 3345- arXiv:1901.00596. 3347. doi: 10.1093/bioinformatics/btw475 Xingjian, S., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-K., and Woo, Sundermeyer, M., Schlüter, R., and Ney, H. (2012). "LSTM neural networks for W.-C. (2015). "Convolutional lstm network: a machine learning approach language modeling," in Thirteenth Annual Conference of the International for precipitation nowcasting," in Advances in Neural Information Processing Speech Communication Association. Systems, 802-810. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015). Yang, Z., Dehmer, M., Yli-Harja, O., and Emmert-Streib, F. (2020). Combining "Going deeper with convolutions," in Proceedings of the IEEE Conference on deep learning with token selection for patient phenotyping from electronic Computer Vision and Pattern Recognition, 1-9. health records. Sci. Rep. 10:1432. doi: 10.1038/s41598-020-58178-1 Theano Development Team (2016). Theano: a Python framework for Yao, L., Mao, C., and Luo, Y. (2019). "Graph convolutional networks for text fast computation of mathematical expressions. arXiv [Preprint]. classification," in Proceedings of the AAAI Conference on Artificial Intelligence, arXiv:abs/1605.02688. Vol. 33, 7370-7377. Tosic, I., and Frossard, P. (2011). Dictionary learning. IEEE Signal Process. Mag. Yin, W., Kann, K., Yu, M., and Schütze, H. (2017). Comparative study of cnn and 28,27-38. rnn for natural language processing. arXiv [Preprint]. arXiv:1702.01923. Venkataraman, S., Yang, Z., Liu, D., Liang, E., Falaki, H., Meng, X., et al. Yoshua, B. (2009). Learning deep architectures for AI. Foundat. Trends Mach. (2016). "Sparkr: Scaling R programs with spark," in Proceedings of Learn. 2, 1-127. doi: 10.1561/2200000006 the 2016 International Conference on Management of Data, SIGMOD Young, T., Hazarika, D., Poria, S., and Cambria, E. (2018). Recent trends in deep '16 (New York, NY: ACM), 1099-1104. doi: 10.1145/2882903. learning based natural language processing. IEEE Comput. Intell. Mag. 13, 2903740 55-75. doi: 10.1109/MCI.2018.2840738 Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P. -A. (2010). Yu, D., and Li, J. (2017). Recent progresses in deep learning based acoustic models. Stacked denoising autoencoders: learning useful representations in a deep IEEE/CAA J. Autom. Sinica 4, 396-409. doi: 10.1109/JAS.2017.7510508 network with a local denoising criterion. J. Mach. Learn. Res. 11, 3371-3408. Zhang, S., Zhou, J., Hu, H., Gong, H., Chen, L., Cheng, C., et al. (2015). A deep Available online at: http://www.jmlr.org/papers/v1l/vincent10a.html learning framework for modeling structural features of rna-binding protein Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). "Show and tell: a neural targets. Nucleic Acids Res. 43:e32. doi: 10.1093/nar/gkv1025 image caption generator," in Proceedings of the IEEE Conference on Computer Zhang, X., Zhao, J., and LeCun, Y. (2015). "Character-level convolutional networks Vision and Pattern Recognition, 3156-3164. for text classification," in Advances in Neural Information Processing Systems, Wan, L., Zeiler, M., Zhang, S., Cun, Y. L., and Fergus, R. (2013). "Regularization 649-657. of neural networks using dropconnect," in Proceedings of the 30th International Zhou, Y. (2019). Sentiment classification with deep neural networks (Master's Conference on Machine Learning (ICML-13) 1058-1066. thesis). Tampere University, Tampere, Finland. Wang, D., and Terman, D. (1995). Locally excitatory globally inhibitory oscillator networks. IEEE Trans. Neural Netw. 6, 283-286. Conflict of Interest: The authors declare that the research was conducted in the Wang, D., and Terman, D. (1997). Image segmentation based on oscillatory absence of any commercial or financial relationships that could be construed as a correlation. Neural Comput. 9, 805-836. potential conflict of interest. Wang, D. L., and Brown, G. J. (1999). Separation of speech from interfering sounds based on oscillatory correlation. IEEE Trans. Neural Netw. 10, Copyright © 2020 Emmert-Streib, Yang, Feng, Tripathi and Dehmer. This is an 684-697. open-access article distributed under the terms of the Creative Commons Attribution Wang, Y., Huang, M., Zhao, L., et al. (2016). "Attention-based lstm for aspect-level License (CC BY). The use, distribution or reproduction in other forums is permitted, sentiment classification," in Proceedings of the 2016 Conference on Empirical provided the original author(s) and the copyright owner(s) are credited and that the Methods in Natural Language Processing, 606-615. original publication in this journal is cited, in accordance with accepted academic Webb, A. R., and Copsey, K. D. (2011). Statistical Pattern Recognition. 3rd Edn. practice. No use, distribution or reproduction is permitted which does not comply Wiley. with these terms. Frontiers in Artificial Intelligence | www.frontiersin.org 23 February 2020 | Volume 3 Article 4 